{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aryangoel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "file_path = '/Users/aryangoel/Desktop/UCR/Fall 2024/CS173/CS173-published-sheet.xlsx'\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove lexicon columns\n",
    "relevant_columns = df.columns[1::2]\n",
    "df_filtered = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence, removes English stop words, and joins the tokens back into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence (str): The sentence to preprocess, represented as a string.\n",
    "\n",
    "    Returns:\n",
    "    - str: The preprocessed sentence, with stop words removed and tokens joined back into a single string.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = re.findall(r'\\b\\w+\\b|[.!?]', sentence)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)  # Join tokens back into a single string for DataFrame storage\n",
    "\n",
    "# Apply preprocessing to each column in the DataFrame\n",
    "processed_df = filtered_df.copy()\n",
    "for col in df_filtered.columns:\n",
    "    processed_df[col] = df_filtered[col].fillna('').apply(preprocess_sentence)  # Fill NaNs with empty strings\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, train_size, validation_size, test_size):\n",
    "    \"\"\"\n",
    "    Splits the given DataFrame into training, validation, and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to split.\n",
    "    - train_size (int): Number of rows for the training set.\n",
    "    - validation_size (int): Number of rows for the validation set.\n",
    "    - test_size (int): Number of rows for the testing set.\n",
    "\n",
    "    Returns:\n",
    "    - train (pd.DataFrame): The training set.\n",
    "    - validation (pd.DataFrame): The validation set.\n",
    "    - test (pd.DataFrame): The testing set.\n",
    "    \"\"\"\n",
    "\n",
    "    if train_size + validation_size + test_size > len(df):\n",
    "        raise ValueError(\"Sum of sizes exceeds DataFrame length.\")\n",
    "    \n",
    "    train_df = df.iloc[0:train_size]\n",
    "    validation_df = df.iloc[train_size:train_size + validation_size]\n",
    "    test_df = df.iloc[train_size + validation_size:train_size + validation_size + test_size]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = split_dataframe(processed_df, 29, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emotion_lists(df, emotions):\n",
    "    \"\"\"\n",
    "    Creates separate lists for each emotion by concatenating relevant columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe with the emotional sentences.\n",
    "    - emotions (list): A list of emotion names to create separate lists for.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are emotion names and values are the corresponding lists of sentences.\n",
    "    \"\"\"\n",
    "    emotion_lists = {emotion: [] for emotion in emotions}\n",
    "\n",
    "    for emotion in emotions:\n",
    "        for column in df.columns:\n",
    "            if emotion in column:\n",
    "                emotion_lists[emotion].extend(df[column].tolist())\n",
    "\n",
    "    return emotion_lists\n",
    "\n",
    "emotions = ['Fear', 'Anger', 'Surprise', 'Disgust', 'Sadness', 'Joy']\n",
    "train_emotion_lists = create_emotion_lists(train_df, emotions)\n",
    "test_emotion_lists = create_emotion_lists(test_df, emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fear': 0.20140515222482436, 'Anger': 0.1358313817330211, 'Surprise': 0.1288056206088993, 'Disgust': 0.13114754098360656, 'Sadness': 0.20140515222482436, 'Joy': 0.20140515222482436}\n"
     ]
    }
   ],
   "source": [
    "# Calculate Prior Probabilities\n",
    "emotion_counts = {emotion: len([s for s in train_emotion_lists[emotion] if s]) for emotion in emotions}\n",
    "total_sentence_count = sum(emotion_counts.values())\n",
    "prior_probabilities = {emotion: emotion_counts[emotion]/total_sentence_count for emotion in emotions}\n",
    "print(prior_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fear': {'As': 0.000462962962962963, 'she': 0.000462962962962963, 'hugged': 0.000462962962962963, 'her': 0.000462962962962963, 'daughter': 0.000925925925925926, 'goodbye': 0.000462962962962963, 'on': 0.000462962962962963, 'the': 0.000462962962962963, 'first': 0.000925925925925926, 'day': 0.000462962962962963, 'of': 0.000462962962962963, 'college,': 0.000462962962962963, 'felt': 0.006944444444444444, 'both': 0.000462962962962963, 'sad': 0.000462962962962963, 'to': 0.000462962962962963, 'see': 0.0023148148148148147, 'go': 0.000925925925925926, 'and': 0.000462962962962963, 'joyful': 0.000462962962962963, 'knowing': 0.000925925925925926, 'that': 0.000462962962962963, 'was': 0.000462962962962963, 'embarking': 0.000462962962962963, 'a': 0.000462962962962963, 'new': 0.001388888888888889, 'exciting': 0.000462962962962963, 'chapter': 0.000462962962962963, 'in': 0.000462962962962963, 'life.': 0.000462962962962963}, 'Anger': {'As': 0.000779423226812159, 'she': 0.000779423226812159, 'hugged': 0.000779423226812159, 'her': 0.000779423226812159, 'daughter': 0.000779423226812159, 'goodbye': 0.000779423226812159, 'on': 0.000779423226812159, 'the': 0.000779423226812159, 'first': 0.000779423226812159, 'day': 0.001558846453624318, 'of': 0.000779423226812159, 'college,': 0.000779423226812159, 'felt': 0.002338269680436477, 'both': 0.000779423226812159, 'sad': 0.000779423226812159, 'to': 0.000779423226812159, 'see': 0.001558846453624318, 'go': 0.002338269680436477, 'and': 0.000779423226812159, 'joyful': 0.000779423226812159, 'knowing': 0.002338269680436477, 'that': 0.000779423226812159, 'was': 0.000779423226812159, 'embarking': 0.000779423226812159, 'a': 0.000779423226812159, 'new': 0.000779423226812159, 'exciting': 0.000779423226812159, 'chapter': 0.000779423226812159, 'in': 0.000779423226812159, 'life.': 0.000779423226812159}, 'Surprise': {'As': 0.000847457627118644, 'she': 0.000847457627118644, 'hugged': 0.000847457627118644, 'her': 0.000847457627118644, 'daughter': 0.000847457627118644, 'goodbye': 0.000847457627118644, 'on': 0.000847457627118644, 'the': 0.000847457627118644, 'first': 0.001694915254237288, 'day': 0.000847457627118644, 'of': 0.000847457627118644, 'college,': 0.000847457627118644, 'felt': 0.00423728813559322, 'both': 0.000847457627118644, 'sad': 0.000847457627118644, 'to': 0.000847457627118644, 'see': 0.003389830508474576, 'go': 0.001694915254237288, 'and': 0.000847457627118644, 'joyful': 0.000847457627118644, 'knowing': 0.000847457627118644, 'that': 0.000847457627118644, 'was': 0.000847457627118644, 'embarking': 0.000847457627118644, 'a': 0.000847457627118644, 'new': 0.000847457627118644, 'exciting': 0.000847457627118644, 'chapter': 0.000847457627118644, 'in': 0.000847457627118644, 'life.': 0.000847457627118644}, 'Disgust': {'As': 0.0008410428931875525, 'she': 0.0008410428931875525, 'hugged': 0.0008410428931875525, 'her': 0.0008410428931875525, 'daughter': 0.0008410428931875525, 'goodbye': 0.0008410428931875525, 'on': 0.0008410428931875525, 'the': 0.0008410428931875525, 'first': 0.001682085786375105, 'day': 0.001682085786375105, 'of': 0.0008410428931875525, 'college,': 0.0008410428931875525, 'felt': 0.005887300252312868, 'both': 0.0008410428931875525, 'sad': 0.0008410428931875525, 'to': 0.0008410428931875525, 'see': 0.002523128679562658, 'go': 0.0008410428931875525, 'and': 0.0008410428931875525, 'joyful': 0.0008410428931875525, 'knowing': 0.0008410428931875525, 'that': 0.0008410428931875525, 'was': 0.0008410428931875525, 'embarking': 0.0008410428931875525, 'a': 0.0008410428931875525, 'new': 0.0008410428931875525, 'exciting': 0.0008410428931875525, 'chapter': 0.0008410428931875525, 'in': 0.0008410428931875525, 'life.': 0.0008410428931875525}, 'Sadness': {'As': 0.0004889975550122249, 'she': 0.0004889975550122249, 'hugged': 0.0004889975550122249, 'her': 0.0004889975550122249, 'daughter': 0.0009779951100244498, 'goodbye': 0.0004889975550122249, 'on': 0.0004889975550122249, 'the': 0.0004889975550122249, 'first': 0.0024449877750611247, 'day': 0.0009779951100244498, 'of': 0.0004889975550122249, 'college,': 0.0004889975550122249, 'felt': 0.008312958435207823, 'both': 0.0004889975550122249, 'sad': 0.0009779951100244498, 'to': 0.0004889975550122249, 'see': 0.00293398533007335, 'go': 0.0004889975550122249, 'and': 0.0004889975550122249, 'joyful': 0.0004889975550122249, 'knowing': 0.001466992665036675, 'that': 0.0004889975550122249, 'was': 0.0004889975550122249, 'embarking': 0.0004889975550122249, 'a': 0.0004889975550122249, 'new': 0.0024449877750611247, 'exciting': 0.0004889975550122249, 'chapter': 0.0004889975550122249, 'in': 0.0004889975550122249, 'life.': 0.0004889975550122249}, 'Joy': {'As': 0.00046490004649000463, 'she': 0.00046490004649000463, 'hugged': 0.00046490004649000463, 'her': 0.00046490004649000463, 'daughter': 0.001394700139470014, 'goodbye': 0.00046490004649000463, 'on': 0.00046490004649000463, 'the': 0.00046490004649000463, 'first': 0.0023245002324500234, 'day': 0.00046490004649000463, 'of': 0.00046490004649000463, 'college,': 0.00046490004649000463, 'felt': 0.00604370060437006, 'both': 0.00046490004649000463, 'sad': 0.00046490004649000463, 'to': 0.00046490004649000463, 'see': 0.0041841004184100415, 'go': 0.00046490004649000463, 'and': 0.00046490004649000463, 'joyful': 0.00046490004649000463, 'knowing': 0.0009298000929800093, 'that': 0.00046490004649000463, 'was': 0.00046490004649000463, 'embarking': 0.00046490004649000463, 'a': 0.00046490004649000463, 'new': 0.0032543003254300326, 'exciting': 0.00046490004649000463, 'chapter': 0.00046490004649000463, 'in': 0.00046490004649000463, 'life.': 0.00046490004649000463}}\n"
     ]
    }
   ],
   "source": [
    "def word_likelihoods(test_sentence: str, document: list):\n",
    "    \"\"\"\n",
    "    Calculates likelihood probabilities for every word in the test_sentence (uses add-one smoothing)\n",
    "\n",
    "    Parameters:\n",
    "    - test_sentence (str): The test sentence with words seperated by spaces.\n",
    "    - document (list): A list of sentences of a particular emotion.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are the words from the test sentence and values are the corresponding likelihood probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    words = test_sentence.split()    \n",
    "    word_count = {}\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "    \n",
    "    # Count occurrences of each word in the document\n",
    "    for sentence in document:\n",
    "        for word in sentence.split():\n",
    "            total_words += 1\n",
    "            unique_words.add(word)\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "                \n",
    "    vocab = len(unique_words)\n",
    "    \n",
    "    # Calculate probabilities for each word in the test sentence\n",
    "    probabilities = {}\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            probabilities[word] = (word_count[word] + 1) / (total_words + vocab)\n",
    "        else:\n",
    "            probabilities[word] = 1 / (total_words + vocab)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def calc_likelihood_prob(sentence):\n",
    "    return {emotion: word_likelihoods(sentence, train_emotion_lists[emotion]) for emotion in emotions}\n",
    "\n",
    "# Example usage\n",
    "sentence = \"As she hugged her daughter goodbye on the first day of college, she felt both sad to see her go and joyful knowing that she was embarking on a new and exciting chapter in her life.\"\n",
    "likelihood_probabilities = calc_likelihood_prob(sentence)\n",
    "print(likelihood_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fear': -223.74965261287713, 'Anger': -212.02290071402712, 'Surprise': -209.86551571711357, 'Disgust': -210.02665242437368, 'Sadness': -219.27512630278184, 'Joy': -221.7037991294918}\n",
      "-209.86551571711357\n",
      "Surprise\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes_probability(emotions, prior_probabilities, likelihood_probabilities):\n",
    "    \"\"\"\n",
    "    Calculates the naive bayes probability for every emotion using the prior and liklihood probabilities\n",
    "\n",
    "    Parameters:\n",
    "    - emotions (list): List of all 6 emotions\n",
    "    - prior_probabilities (dict): A dictionary where keys are the emotions and values are the prior probabilities\n",
    "    - likelihood_probabilities (dict{dict}): A dictionary of dictionaries where keys are the emotions and values are dictionaries of the likelihood probabilities of each word in the test sentence\n",
    "\n",
    "    Returns:\n",
    "    - emotion (str): The emotion with the highest naive bayes probability\n",
    "    \"\"\"\n",
    "    \n",
    "    naive_bayes_probabilities = {}\n",
    "\n",
    "    for emotion in emotions:\n",
    "        prior = prior_probabilities[emotion]\n",
    "\n",
    "        likelihood_product = 0\n",
    "        \n",
    "        for word, likelihood in likelihood_probabilities[emotion].items():\n",
    "            likelihood_product += math.log(likelihood)\n",
    "\n",
    "        joint_probability = math.log(prior) + likelihood_product\n",
    "        \n",
    "        naive_bayes_probabilities[emotion] = joint_probability\n",
    "        \n",
    "    print(naive_bayes_probabilities)\n",
    "    print(naive_bayes_probabilities[highest_emotion])\n",
    "    \n",
    "    return max(naive_bayes_probabilities, key=naive_bayes_probabilities.get)\n",
    "\n",
    "highest_emotion = naive_bayes_probability(emotions, prior_probabilities, likelihood_probabilities)\n",
    "print(highest_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X  Fear  Anger  Surprise  Disgust  Sadness  Joy\n",
      "0  Fear      1     0      1         1        1        1  \n",
      "1  Anger     8     8      1         2        8        6  \n",
      "2  Surprise  11    7      15        6        6        12 \n",
      "3  Disgust   8     5      3         11       8        8  \n",
      "4  Sadness   1     0      0         0        6        2  \n",
      "5  Joy       1     0      0         0        1        1  \n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix\n",
    "predictions_list = []\n",
    "matrix_col = []\n",
    "confusion_matrix_data = {'X': ['Fear', 'Anger', 'Surprise', 'Disgust', 'Sadness', 'Joy'], 'Fear': [0,0,0,0,0,0], 'Anger': [0,0,0,0,0,0], 'Surprise': [0,0,0,0,0,0], 'Disgust': [0,0,0,0,0,0], 'Sadness': [0,0,0,0,0,0], 'Joy': [0,0,0,0,0,0]}\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix_data)\n",
    "confusion_matrix\n",
    "for emotion in emotions:\n",
    "    for test_sentence in test_emotion_lists[emotion]:\n",
    "        predictions_list.append(naive_bayes_probability(emotions, prior_probabilities, calc_likelihood_prob(test_sentence)))\n",
    "    matrix_col.append(predictions_list.count('Fear'))\n",
    "    matrix_col.append(predictions_list.count('Anger'))\n",
    "    matrix_col.append(predictions_list.count('Surprise'))\n",
    "    matrix_col.append(predictions_list.count('Disgust'))\n",
    "    matrix_col.append(predictions_list.count('Sadness'))\n",
    "    matrix_col.append(predictions_list.count('Joy'))\n",
    "    confusion_matrix[emotion] = matrix_col\n",
    "    predictions_list = []\n",
    "    matrix_col = []\n",
    "\n",
    "print(confusion_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       0.88      1.00      0.94        30\n",
      "           2       1.00      0.83      0.91        24\n",
      "\n",
      "    accuracy                           0.95        75\n",
      "   macro avg       0.96      0.94      0.95        75\n",
      "weighted avg       0.95      0.95      0.95        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report  # Import the classification report function\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
